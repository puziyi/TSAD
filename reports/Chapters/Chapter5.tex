% Chapter 5

\chapter{Conclusion and Future Work} % Main chapter title

\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

\section{Conclusion}
In our project, we first reviewed different existing research on sequence modeling methods and found their characteristics. Then we used common machine learning training datasets to check the availability of TCN and ascertain its power over other sequential models. Next, we checked to predict the ability of TCN over other relatively traditional sequential models, such as Gated recurrent units (GRUs), long short-term memory (LSTM), vanilla RNN, and False nearest neighbor (FNN),  by using some specific sequence data. And in order to have a better result, we tune the hyperparameters in the TCN model. After comparison, TCN showed itself as both robust and accurate even amongst deep learning models. 

However, it can not demonstrate convincing performance. Next, we try to discover the data sensitivity of the TCN model and add the LSTM model as a comparison. We find that TCN performs better than LSTM in fluctuated data. And for short data size, TCN needs more time to attain a good result, while for long and huge data, TCN has the ability to perform better. As TCN is sensitive to the size of datasets, we adjust the TCN's convolutional kernel to increase the receptive field size, which improves the final performance significantly and increases training efficiency. Besides, we try to upgrade our TCN model by importing an attention mechanism to decrease the computational cost.

\section{Further Work}
However, our group still found some limitations and hope to do more improvements on following aspects:
\begin{itemize}
    \item Limited dataset. In this project, we tried several different datasets and chose Gefcom2014 electronic data as an object dataset. The Gefcom2014 data have some good features such as regular and smooth. However, it also brings a limitation to model fitting and makes the experiment not that convincing. And we also need more types of data to verify the characteristics we found in TCN. So it's better for us to try to do more tests on the finding of our project with more grid load datasets, for example, from different countries or states.
    \item Higher predict accuracy with lower computational cost. During this project, we mainly focus on the prediction accuracy in the anomaly period. However,  one significant drawback is that many computational resources, time-consuming and high memory and CPU occupated. Although we have modified convolution kernels and network structures and indeed improve a lot, we plan to further chase higher and more stable predict accuracy in a cost-saving manner. 
    \item Integrate into reality application situations. As we mentioned before, grid load prediction and anomaly detection methods could be used in various situations. Hence, if possible, we want to integrate our model into certain situations and make real-time predictions for this electronic society.
\end{itemize}